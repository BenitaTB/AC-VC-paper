{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa33ade-818a-4013-8510-9a0853617835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from scipy import stats\n",
    "import statsmodels.stats.multitest\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "import pims\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4fcaa-5cd1-4c74-adf8-38c0301690a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doHierarchicalBoostrap(df, data, areas,nBoot = 1000, nAnimals = 4, nRois = 200, dataType = 'locations'):\n",
    "  \n",
    "    nAreas = len(areas)   \n",
    "\n",
    "    data_byArea = []\n",
    "    animals_byArea = []\n",
    "    for ar in range(nAreas):\n",
    "        idx_thisArea = np.nonzero(np.array(df['area']) == areas[ar])[0]\n",
    "       \n",
    "        data_thisArea = data[idx_thisArea]\n",
    "        df_thisArea = df.iloc[idx_thisArea]\n",
    "\n",
    "        data_byAnimal = []\n",
    "        an = []\n",
    "        theseAnimals = np.unique(np.array(df_thisArea['animal']))\n",
    "        for a in range(len(theseAnimals)):\n",
    "            idx_thisAnimal = np.nonzero(np.array(df_thisArea['animal']) == theseAnimals[a])[0]\n",
    "           \n",
    "            data_thisAnimal = data_thisArea[idx_thisAnimal]\n",
    "           \n",
    "            if len(data_thisAnimal) >= 20:\n",
    "                data_byAnimal.append(data_thisAnimal)\n",
    "                an.append(theseAnimals[a])\n",
    "       \n",
    "        data_byArea.append(data_byAnimal)\n",
    "        animals_byArea.append(np.array(an))\n",
    "   \n",
    "    this = []\n",
    "    for i in range(10):\n",
    "        for j in range(len(data_byArea[i])):\n",
    "            this.append(len(data_byArea[i][j]))\n",
    "        \n",
    "   \n",
    "    ### for each pair of areas, make the bootstrap samples and then do a ks-test\n",
    "    pairs = list(itertools.combinations(range(0, nAreas), 2))\n",
    "    ks_boot = np.empty((len(pairs), nBoot))\n",
    "    rk_boot = np.empty((len(pairs), nBoot))\n",
    "    rk_boot_shuffle = np.empty((len(pairs), nBoot))\n",
    "    median_diff = np.empty((len(pairs), nBoot))\n",
    "\n",
    "    sampleLength1, sampleLength2 = [],[]\n",
    "\n",
    "    for i in tqdm(range(len(pairs))):\n",
    "\n",
    "        data1 = data_byArea[pairs[i][0]]\n",
    "        data2 = data_byArea[pairs[i][1]]\n",
    "        sample1, sample2 =[], []\n",
    "        for k in range(nBoot):\n",
    "            #first area\n",
    "            n1 = len(data1)\n",
    "            \n",
    "            boot_data1 = np.empty((0,))\n",
    "            while len(boot_data1) ==0:\n",
    "                # if n1 >= nAnimals:\n",
    "                boot_an1 = np.random.choice(np.arange(0,n1), nAnimals, replace =True)\n",
    "                # else:\n",
    "                    # boot_an1 = np.random.choice(np.arange(0,n1), n1, replace =True)\n",
    "\n",
    "\n",
    "                # for j in range(nAnimals):\n",
    "                for j in range(len(boot_an1)):\n",
    "\n",
    "                    data1_this = data1[boot_an1[j]]\n",
    "                    \n",
    "                    boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), nRois, replace =True)\n",
    "\n",
    "                    # if len(data1_this) > nRois:\n",
    "                    #     boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), nRois, replace =True)\n",
    "                    # else:\n",
    "                    #     boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), len(data1_this), replace =True)\n",
    "\n",
    "                    # if len(data1_this) > nRois:\n",
    "                    #     boot_rois1 = np.random.randint(len(data1_this), size=nRois)\n",
    "                    # else:\n",
    "                    #     boot_rois1 = np.random.randint(len(data1_this), size=int(len(data1_this)/2))\n",
    "    \n",
    "                    data_forBoot = np.squeeze(data1_this[boot_rois1])\n",
    "    \n",
    "                    # boot_data2 = np.concatenate((boot_data2, data2_this[boot_rois2]))\n",
    "                    if len(data_forBoot.shape) ==0:\n",
    "                        data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "                  \n",
    "                    boot_data1 = np.concatenate((boot_data1,data_forBoot))\n",
    "           \n",
    "            sample1.append(len(boot_data1))\n",
    "           \n",
    "            #second area\n",
    "            n2 = len(data2)\n",
    "            boot_data2 = np.empty((0,))\n",
    "\n",
    "            while len(boot_data2) ==0:\n",
    "                # if n2 >= nAnimals:\n",
    "                boot_an2 = np.random.choice(np.arange(0,n2), nAnimals, replace =True)\n",
    "                # else:\n",
    "                    # boot_an2 = np.random.choice(np.arange(0,n2), n2, replace =True)\n",
    "\n",
    "                # for j in range(nAnimals):\n",
    "                for j in range(len(boot_an2)):\n",
    "\n",
    "                    data2_this = data2[boot_an2[j]]\n",
    "                    \n",
    "                    boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), nRois, replace =True)\n",
    "\n",
    "                    # if len(data2_this) > nRois:\n",
    "                    #     boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), nRois, replace =True)\n",
    "                    # else:\n",
    "                    #     boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), len(data2_this), replace =True)\n",
    "\n",
    "                    # if len(data2_this) > nRois:\n",
    "                    #     boot_rois2 = np.random.randint(len(data2_this), size=nRois)\n",
    "                    # else:\n",
    "                    #     boot_rois2 = np.random.randint(len(data2_this), size=int(len(data2_this)/2))\n",
    "    \n",
    "                    data_forBoot = np.squeeze(data2_this[boot_rois2])\n",
    "    \n",
    "                    # boot_data2 = np.concatenate((boot_data2, data2_this[boot_rois2]))\n",
    "                    if len(data_forBoot.shape) ==0:\n",
    "                        data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "                  \n",
    "                    boot_data2 = np.concatenate((boot_data2,data_forBoot))\n",
    "         \n",
    "     \n",
    "            sample2.append(len(boot_data2))\n",
    "\n",
    "\n",
    "            D, p = stats.kstest(boot_data1,boot_data2)           \n",
    "            ks_boot[i,k] = D\n",
    "            \n",
    "            U, p = stats.mannwhitneyu(boot_data1,boot_data2)    \n",
    "            rk_boot[i,k] = U\n",
    "            \n",
    "            median_diff[i,k] = abs(np.median(boot_data1) - np.median(boot_data2))\n",
    "\n",
    "            # joint_boot = np.concatenate((boot_data1, boot_data2),0)\n",
    "            # random.shuffle(joint_boot)\n",
    "            # boot_data1_sh = joint_boot[:len(boot_data1)]\n",
    "            # boot_data2_sh = joint_boot[len(boot_data1):]\n",
    "            # U, p = stats.mannwhitneyu(boot_data1_sh,boot_data2_sh)    \n",
    "            # rk_boot_shuffle[i,k] = U\n",
    "                        \n",
    "      \n",
    "        sampleLength1.append(np.median(np.array(sample1)))   \n",
    "        sampleLength2.append(np.median(np.array(sample2)))   \n",
    "\n",
    "    #################################################################   \n",
    "    #check significance of ks test for each pair   \n",
    "    #ref: http://fcaglp.unlp.edu.ar/~observacional/papers/PDFs/statistics/Critical_KS.pdf\n",
    "    #taken from code in HanBonin2022\n",
    "    def get_critical_ranksum(n1, n2, alpha, tail = 'two-sided'):\n",
    "        from scipy.stats import norm\n",
    "\n",
    "        # Mean of the U distribution\n",
    "        mean_u = n1 * n2 / 2\n",
    "        \n",
    "        # Standard deviation of the U distribution\n",
    "        sd_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "    \n",
    "        # Critical z-value for the desired alpha\n",
    "        if tail == 'two-sided':\n",
    "            z_critical = norm.ppf(1 - alpha / 2)\n",
    "        elif tail == 'one-sided':\n",
    "            z_critical = norm.ppf(1 - alpha)\n",
    "        else:\n",
    "            raise ValueError(\"tail must be 'two-sided' or 'one-sided'\")\n",
    "    \n",
    "        # Critical value of U\n",
    "        u_critical_lower = mean_u - z_critical * sd_u\n",
    "        u_critical_upper = mean_u + z_critical * sd_u\n",
    "        \n",
    "        return  u_critical_lower, u_critical_upper\n",
    "    \n",
    "    mat_alpha_coefficient= [[ 0.1, 0.05, .025,.01, .005, .001],[ 1.22, 1.36, 1.48, 1.63, 1.73, 1.95]]\n",
    "   \n",
    "    bins_Da = np.arange(0,1,0.02)\n",
    "    sigLevel_ks = []\n",
    "    for i in range(len(pairs)):\n",
    "        CI_low = np.percentile(ks_boot[i,:], 2.5)\n",
    "        CI_high = np.percentile(ks_boot[i,:], 97.5)\n",
    "\n",
    "        n1 = int(sampleLength1[i])\n",
    "        n2 = int(sampleLength2[i])\n",
    "       \n",
    "        vec_Da=[mat_alpha_coefficient[1][j] * np.sqrt((n1+n2)/(n1*n2)) for j in range(len(mat_alpha_coefficient[1]))]\n",
    "\n",
    "        if CI_low > vec_Da[5]:\n",
    "            sigLevel_ks.append(3)\n",
    "        elif CI_low > vec_Da[3]:\n",
    "            sigLevel_ks.append(2)\n",
    "        elif CI_low > vec_Da[1]:\n",
    "            sigLevel_ks.append(1)\n",
    "        else:\n",
    "            sigLevel_ks.append(0)\n",
    "    \n",
    "      \n",
    "    bins_rk = np.arange(1000,20000,1000)\n",
    "    alphas = [0.1, 0.05, 0.01, 0.001]\n",
    "    sigLevel_mannU= []\n",
    "    for i in range(len(pairs)):\n",
    "    \n",
    "        CI_low = np.percentile(rk_boot[i,:], 2.5)\n",
    "        CI_high = np.percentile(rk_boot[i,:], 97.5)\n",
    "        n1 = int(sampleLength1[i])\n",
    "        n2 = int(sampleLength2[i])\n",
    "        \n",
    "        crit =[get_critical_ranksum(n1,n2, alphas[i]) for i in range(len(alphas))]\n",
    "\n",
    "        if CI_low > crit[3][0]:\n",
    "            sigLevel_mannU.append(3)\n",
    "        elif CI_low > crit[2][0]:\n",
    "            sigLevel_mannU.append(2)\n",
    "        elif CI_low > crit[1][0]:\n",
    "            sigLevel_mannU.append(1)\n",
    "        else:\n",
    "            sigLevel_mannU.append(0)\n",
    "            \n",
    "    ks_sigLevels_mat = np.empty((len(areas), len(areas))); ks_sigLevels_mat[:] = np.nan\n",
    "    ks_distance_mat = np.empty((len(areas), len(areas))); ks_distance_mat[:] = np.nan\n",
    "    median_dist_mat = np.empty((len(areas), len(areas))); median_dist_mat[:] = np.nan\n",
    "    mannU_sigLevels_mat = np.empty((len(areas), len(areas))); mannU_sigLevels_mat[:] = np.nan\n",
    "    for i in range(len(pairs)):\n",
    "        pos0 = pairs[i][0]\n",
    "        pos1 = pairs[i][1]\n",
    "        ks_sigLevels_mat[pos0,pos1] = sigLevel_ks[i]\n",
    "        ks_distance_mat[pos1, pos0] =  np.nanmedian(ks_boot[i,:])\n",
    "        mannU_sigLevels_mat[pos0,pos1] = sigLevel_mannU[i]\n",
    "        median_dist_mat[pos1,pos0] = np.nanmedian(median_diff[i,:])\n",
    "       \n",
    "    return ks_distance_mat, ks_sigLevels_mat, mannU_sigLevels_mat, median_dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93dc38-f03e-431d-974f-85d0c79f8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doHierarchicalBoostrap_byStream(df, data, groups,nBoot = 1000, nAnimals = 5, nRois = 100, dataType = 'locations'):\n",
    "    import itertools\n",
    "    import random\n",
    "   \n",
    "    \n",
    "    nGroups = len(groups)\n",
    "    \n",
    "    data_byGroup = []\n",
    "    animals_byArea = []\n",
    "    for g in range(nGroups):\n",
    "        idx_thisGroup = np.nonzero(np.array(df['streamIdx']) == groups[g])[0]\n",
    "       \n",
    "        data_thisArea = data[idx_thisGroup]\n",
    "        df_thisArea = df.iloc[idx_thisGroup]\n",
    "\n",
    "        data_byAnimal = []\n",
    "        an = []\n",
    "        theseAnimals = np.unique(np.array(df_thisArea['animal']))\n",
    "        for a in range(len(theseAnimals)):\n",
    "            idx_thisAnimal = np.nonzero(np.array(df_thisArea['animal']) == theseAnimals[a])[0]\n",
    "           \n",
    "            data_thisAnimal = data_thisArea[idx_thisAnimal]\n",
    "           \n",
    "            if len(data_thisAnimal) >= 20:\n",
    "                data_byAnimal.append(data_thisAnimal)\n",
    "                an.append(theseAnimals[a])\n",
    "       \n",
    "        data_byGroup.append(data_byAnimal)\n",
    "        animals_byArea.append(np.array(an))\n",
    "   \n",
    "    this = []\n",
    "    for i in range(3):\n",
    "        for j in range(len(data_byGroup[i])):\n",
    "            this.append(len(data_byGroup[i][j]))\n",
    "        \n",
    "    # nBoot = 1000\n",
    "    # nAnimals = 5\n",
    "    # nRois = 100\n",
    "   \n",
    "    # plt.close('all')\n",
    "    # myBins = np.arange(0,11,0.5)\n",
    "    # hist_all, bins = np.histogram(data,myBins)\n",
    "    # hist_all_norm = hist_all/np.sum(hist_all)\n",
    "    \n",
    "    # #plot hist for each animal so we see what we are working with\n",
    "    # for ar in range(len(areas)):\n",
    "    #     fig = plt.figure(figsize=(14,10), constrained_layout =True)\n",
    "    #     plt.suptitle(areas[ar])\n",
    "    #     for an in range(len(data_byArea[ar])):\n",
    "    #         ax = fig.add_subplot(5,4,an+1)\n",
    "            \n",
    "    #         hist_thisArea, bins = np.histogram(data_byArea[ar][an],myBins)\n",
    "    #         hist_thisArea_norm = hist_thisArea/np.sum(hist_thisArea)\n",
    "                        \n",
    "    #         plt.hist(bins[:-1],bins,weights = hist_thisArea_norm, color = 'b',histtype='stepfilled', alpha = 0.7,label = 'n: ' + str(len(data_byArea[ar][an])))           \n",
    "    #         plt.hist(bins[:-1],bins,weights = hist_thisArea_norm, color = 'b',histtype='step',linewidth = 1.5, alpha = 1)           \n",
    "    #         # plt.hist(bins[:-1],bins,weights = hist_thisArea_norm, color = '#2D95FC',histtype='stepfilled', alpha = 0.7,label = 'n: ' + str(len(gaussIdx_thisArea)))           \n",
    "    #         # plt.hist(bins[:-1],bins,weights = hist_thisArea_norm, color = '#2D95FC',histtype='step',linewidth = 3, alpha = 1,label = 'n: ' + str(len(gaussIdx_thisArea)))\n",
    "            \n",
    "    #         plt.hist(bins[:-1],bins,weights = hist_all_norm, color = 'k', histtype ='step', linewidth = 0.7)\n",
    "    #         plt.title(str(len(data_byArea[ar][an])) + ', A' + str(animals_byArea[ar][an]))\n",
    "         \n",
    "        \n",
    "   \n",
    "    ### for each pair of areas, make the bootstrap samples and then do a ks-test\n",
    "    pairs = list(itertools.combinations(range(0, nGroups), 2))\n",
    "    ks_boot = np.empty((len(pairs), nBoot))\n",
    "    rk_boot = np.empty((len(pairs), nBoot))\n",
    "    rk_boot_shuffle = np.empty((len(pairs), nBoot))\n",
    "    median_diff = np.empty((len(pairs), nBoot))\n",
    "\n",
    "    sampleLength1, sampleLength2 = [],[]\n",
    "    for i in tqdm(range(len(pairs))):\n",
    "        data1 = data_byGroup[pairs[i][0]]\n",
    "        data2 = data_byGroup[pairs[i][1]]\n",
    "        sample1, sample2 =[],[]\n",
    "        for k in range(nBoot):\n",
    "            #first area\n",
    "            n1 = len(data1)\n",
    "            \n",
    "            boot_data1 = np.empty((0,))\n",
    "            while len(boot_data1) ==0:\n",
    "                if n1 >= nAnimals:\n",
    "                    boot_an1 = np.random.choice(np.arange(0,n1), nAnimals, replace =True)\n",
    "                else:\n",
    "                    boot_an1 = np.random.choice(np.arange(0,n1), n1, replace =True)\n",
    "\n",
    "\n",
    "                # for j in range(nAnimals):\n",
    "                for j in range(len(boot_an1)):\n",
    "\n",
    "                    data1_this = data1[boot_an1[j]]\n",
    "                    \n",
    "                    boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), nRois, replace =True)\n",
    "\n",
    "                    # if len(data1_this) > nRois:\n",
    "                    #     boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), nRois, replace =True)\n",
    "                    # else:\n",
    "                    #     boot_rois1 = np.random.choice(np.arange(0,len(data1_this)), len(data1_this), replace =True)\n",
    "\n",
    "                    # if len(data1_this) > nRois:\n",
    "                    #     boot_rois1 = np.random.randint(len(data1_this), size=nRois)\n",
    "                    # else:\n",
    "                    #     boot_rois1 = np.random.randint(len(data1_this), size=int(len(data1_this)/2))\n",
    "    \n",
    "                    data_forBoot = np.squeeze(data1_this[boot_rois1])\n",
    "    \n",
    "                    # boot_data2 = np.concatenate((boot_data2, data2_this[boot_rois2]))\n",
    "                    if len(data_forBoot.shape) ==0:\n",
    "                        data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "                  \n",
    "                    boot_data1 = np.concatenate((boot_data1,data_forBoot))\n",
    "           \n",
    "            sample1.append(len(boot_data1))\n",
    "           \n",
    "            #second area\n",
    "            n2 = len(data2)\n",
    "            boot_data2 = np.empty((0,))\n",
    "\n",
    "            while len(boot_data2) ==0:\n",
    "                if n2 >= nAnimals:\n",
    "                    boot_an2 = np.random.choice(np.arange(0,n2), nAnimals, replace =True)\n",
    "                else:\n",
    "                    boot_an2 = np.random.choice(np.arange(0,n2), n2, replace =True)\n",
    "\n",
    "                # for j in range(nAnimals):\n",
    "                for j in range(len(boot_an2)):\n",
    "\n",
    "                    data2_this = data2[boot_an2[j]]\n",
    "                    \n",
    "                    boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), nRois, replace =True)\n",
    "\n",
    "                    # if len(data2_this) > nRois:\n",
    "                    #     boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), nRois, replace =True)\n",
    "                    # else:\n",
    "                    #     boot_rois2 = np.random.choice(np.arange(0,len(data2_this)), len(data2_this), replace =True)\n",
    "\n",
    "                    # if len(data2_this) > nRois:\n",
    "                    #     boot_rois2 = np.random.randint(len(data2_this), size=nRois)\n",
    "                    # else:\n",
    "                    #     boot_rois2 = np.random.randint(len(data2_this), size=int(len(data2_this)/2))\n",
    "    \n",
    "                    data_forBoot = np.squeeze(data2_this[boot_rois2])\n",
    "    \n",
    "                    # boot_data2 = np.concatenate((boot_data2, data2_this[boot_rois2]))\n",
    "                    if len(data_forBoot.shape) ==0:\n",
    "                        data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "                  \n",
    "                    boot_data2 = np.concatenate((boot_data2,data_forBoot))\n",
    "         \n",
    "     \n",
    "            sample2.append(len(boot_data2))\n",
    "\n",
    "\n",
    "            D, p = stats.kstest(boot_data1,boot_data2)           \n",
    "            ks_boot[i,k] = D\n",
    "            \n",
    "            U, p = stats.mannwhitneyu(boot_data1,boot_data2)    \n",
    "            rk_boot[i,k] = U\n",
    "            \n",
    "            median_diff[i,k] = abs(np.median(boot_data1) - np.median(boot_data2))\n",
    "\n",
    "            # joint_boot = np.concatenate((boot_data1, boot_data2),0)\n",
    "            # random.shuffle(joint_boot)\n",
    "            # boot_data1_sh = joint_boot[:len(boot_data1)]\n",
    "            # boot_data2_sh = joint_boot[len(boot_data1):]\n",
    "            # U, p = stats.mannwhitneyu(boot_data1_sh,boot_data2_sh)    \n",
    "            # rk_boot_shuffle[i,k] = U\n",
    "                        \n",
    "      \n",
    "        sampleLength1.append(np.median(np.array(sample1)))   \n",
    "        sampleLength2.append(np.median(np.array(sample2)))   \n",
    "\n",
    "    #################################################################   \n",
    "    mat_alpha_coefficient= [[ 0.1, 0.05, .025,.01, .005, .001],[ 1.22, 1.36, 1.48, 1.63, 1.73, 1.95]]\n",
    "   \n",
    "    bins_Da = np.arange(0,1,0.02)\n",
    "    sigLevel_ks = []\n",
    "    for i in range(len(pairs)):\n",
    "        CI_low = np.percentile(ks_boot[i,:], 2.5)\n",
    "        CI_high = np.percentile(ks_boot[i,:], 97.5)\n",
    "\n",
    "        n1 = int(sampleLength1[i])\n",
    "        n2 = int(sampleLength2[i])\n",
    "       \n",
    "        vec_Da=[mat_alpha_coefficient[1][j] * np.sqrt((n1+n2)/(n1*n2)) for j in range(len(mat_alpha_coefficient[1]))]\n",
    "\n",
    "        if CI_low > vec_Da[5]:\n",
    "            sigLevel_ks.append(3)\n",
    "        elif CI_low > vec_Da[3]:\n",
    "            sigLevel_ks.append(2)\n",
    "        elif CI_low > vec_Da[1]:\n",
    "            sigLevel_ks.append(1)\n",
    "        else:\n",
    "            sigLevel_ks.append(0)\n",
    "            \n",
    "        # plt.figure()\n",
    "        # plt.hist(ks_boot[i,:], 20)\n",
    "        # plt.vlines(vec_Da[1], 0, 20, color ='r')\n",
    "\n",
    "    # fig = plt.figure() \n",
    "    bins_rk = np.arange(1000,20000,1000)\n",
    "    alphas = [0.1, 0.05, 0.01, 0.001]\n",
    "    sigLevel_mannU= []\n",
    "    for i in range(len(pairs)):\n",
    "    \n",
    "        CI_low = np.percentile(rk_boot[i,:], 2.5)\n",
    "        CI_high = np.percentile(rk_boot[i,:], 97.5)\n",
    "        n1 = int(sampleLength1[i])\n",
    "        n2 = int(sampleLength2[i])\n",
    "        \n",
    "        crit =[get_critical_ranksum(n1,n2, alphas[i]) for i in range(len(alphas))]\n",
    "\n",
    "        if CI_low > crit[3][0]:\n",
    "            sigLevel_mannU.append(3)\n",
    "        elif CI_low > crit[2][0]:\n",
    "            sigLevel_mannU.append(2)\n",
    "        elif CI_low > crit[1][0]:\n",
    "            sigLevel_mannU.append(1)\n",
    "        else:\n",
    "            sigLevel_mannU.append(0)\n",
    "            \n",
    "    \n",
    "    ks_sigLevels_mat = np.empty((nGroups, nGroups)); ks_sigLevels_mat[:] = np.nan\n",
    "    ks_distance_mat = np.empty((nGroups, nGroups)); ks_distance_mat[:] = np.nan\n",
    "    median_dist_mat = np.empty((nGroups, nGroups)); median_dist_mat[:] = np.nan\n",
    "    mannU_sigLevels_mat = np.empty((nGroups, nGroups)); mannU_sigLevels_mat[:] = np.nan\n",
    "    for i in range(len(pairs)):\n",
    "        pos0 = pairs[i][0]\n",
    "        pos1 = pairs[i][1]\n",
    "        ks_sigLevels_mat[pos0,pos1] = sigLevel_ks[i]\n",
    "        ks_distance_mat[pos1, pos0] =  np.nanmedian(ks_boot[i,:])\n",
    "        mannU_sigLevels_mat[pos0,pos1] = sigLevel_mannU[i]\n",
    "        median_dist_mat[pos1,pos0] = np.nanmedian(median_diff[i,:])\n",
    "\n",
    "    return ks_distance_mat, ks_sigLevels_mat, mannU_sigLevels_mat, median_dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e662e28-d6c0-426a-8324-8718757909fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doHierarchicalBoostrap_byInjectionSite(df, data, animalGroups, nBoot = 1000, nAnimals = 5, nRois = 100):\n",
    "    import itertools\n",
    "    import random\n",
    "    \n",
    "    # df = df0\n",
    "    # data = peak0\n",
    "    # animalGroups = [ventralAn, dorsalAn]\n",
    "    \n",
    "    ks_boot = np.empty((nBoot))\n",
    "    rk_boot = np.empty((nBoot))\n",
    "  \n",
    "    sampleLength1, sampleLength2 = [],[]\n",
    "    for k in range(nBoot):\n",
    "        \n",
    "        #first group\n",
    "        boot_an1 = np.random.choice(animalGroups[0], nAnimals, replace =True)\n",
    "        \n",
    "        boot_data1 = np.empty((0,))\n",
    "\n",
    "        for j in range(len(boot_an1)):\n",
    "            idx_thisAn = np.nonzero(np.array(df['animal']) == boot_an1[j])[0]\n",
    "            \n",
    "            boot_rois1 = np.random.choice(idx_thisAn, nRois, replace =True)\n",
    "        \n",
    "            data_forBoot = np.squeeze(data[boot_rois1])\n",
    "\n",
    "            if len(data_forBoot.shape) ==0:\n",
    "                data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "           \n",
    "            boot_data1 = np.concatenate((boot_data1,data_forBoot))\n",
    "\n",
    "        sampleLength1.append(len(boot_data1))\n",
    "       \n",
    "        #second group\n",
    "        boot_an2 = np.random.choice(animalGroups[1], nAnimals, replace =True)\n",
    "        \n",
    "        boot_data2 = np.empty((0,))\n",
    "\n",
    "        for j in range(len(boot_an2)):\n",
    "            idx_thisAn = np.nonzero(np.array(df['animal']) == boot_an2[j])[0]\n",
    "            \n",
    "            boot_rois2 = np.random.choice(idx_thisAn, nRois, replace =True)\n",
    "        \n",
    "            data_forBoot = np.squeeze(data[boot_rois2])\n",
    "\n",
    "            if len(data_forBoot.shape) ==0:\n",
    "                data_forBoot = np.expand_dims(data_forBoot,0)\n",
    "           \n",
    "            boot_data2 = np.concatenate((boot_data2,data_forBoot))\n",
    "\n",
    "        sampleLength2.append(len(boot_data2))\n",
    "\n",
    "\n",
    "        D, p = stats.kstest(boot_data1,boot_data2)           \n",
    "        ks_boot[k] = D\n",
    "        \n",
    "        U, p = stats.mannwhitneyu(boot_data1,boot_data2)    \n",
    "        rk_boot[k] = U\n",
    "        \n",
    "        # median_diff[k] = abs(np.median(boot_data1) - np.median(boot_data2))\n",
    "\n",
    "       \n",
    "    mat_alpha_coefficient= [[ 0.1, 0.05, .025,.01, .005, .001],[ 1.22, 1.36, 1.48, 1.63, 1.73, 1.95]]\n",
    "       \n",
    "    # fig = plt.figure()    \n",
    "    bins_Da = np.arange(0,1,0.02)\n",
    "    sigLevel_ks = []\n",
    "   \n",
    "    CI_low = np.percentile(ks_boot, 2.5)\n",
    "    CI_high = np.percentile(ks_boot, 97.5)\n",
    "\n",
    "    n1 = int(np.nanmean(sampleLength1))\n",
    "    n2 = int(np.nanmean(sampleLength2))\n",
    "       \n",
    "    vec_Da=[mat_alpha_coefficient[1][j] * np.sqrt((n1+n2)/(n1*n2)) for j in range(len(mat_alpha_coefficient[1]))]\n",
    "    \n",
    "    if CI_low > vec_Da[5]:\n",
    "        sigLevel_ks.append(3)\n",
    "    elif CI_low > vec_Da[3]:\n",
    "        sigLevel_ks.append(2)\n",
    "    elif CI_low > vec_Da[1]:\n",
    "        sigLevel_ks.append(1)\n",
    "    else:\n",
    "        sigLevel_ks.append(0)\n",
    "   \n",
    "    # fig = plt.figure() \n",
    "    alphas = [0.1, 0.05, 0.01, 0.001]\n",
    "    sigLevel_mannU= []\n",
    "   \n",
    "    CI_low = np.percentile(rk_boot, 2.5)\n",
    "    CI_high = np.percentile(rk_boot, 97.5)\n",
    "    n1 = int(np.nanmean(sampleLength1))\n",
    "    n2 = int(np.nanmean(sampleLength2)) \n",
    "    \n",
    "    crit =[get_critical_ranksum(n1,n2, alphas[i]) for i in range(len(alphas))]\n",
    "\n",
    "    if CI_low > crit[3][0]:\n",
    "        sigLevel_mannU.append(3)\n",
    "    elif CI_low > crit[2][0]:\n",
    "        sigLevel_mannU.append(2)\n",
    "    elif CI_low > crit[1][0]:\n",
    "        sigLevel_mannU.append(1)\n",
    "    else:\n",
    "        sigLevel_mannU.append(0)                \n",
    "\n",
    "    return sigLevel_ks, sigLevel_mannU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a41f8b-de5e-4c0d-b539-838ab6fa4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSpatialBinnedMap(ref,spatialBin = 100):\n",
    "    \n",
    "    um_per_pixel = 9 #this is from measuring the WF FOV size\n",
    "     \n",
    "    spatialBin_px = int(spatialBin/um_per_pixel)\n",
    "                \n",
    "    # spatialBin = 11 #pixels\n",
    "            \n",
    "    nY,nX = ref.shape[0:2]\n",
    "\n",
    "    nBins_x = np.ceil(nX/spatialBin_px)\n",
    "    nBins_y = np.ceil(nY/spatialBin_px)\n",
    "                    \n",
    "    X,Y = np.meshgrid(np.arange(0,nBins_x), np.arange(0,nBins_y))\n",
    "        \n",
    "    Z = X*100 + Y\n",
    "        \n",
    "    matrix = np.repeat(Z, spatialBin_px, axis=1).repeat(spatialBin_px, axis=0)            \n",
    "    matrix = matrix[0:nY,0:nX]\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5055ef4-15ad-45cb-8072-5b112ade9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataPaths(dataPath, animalPaths):\n",
    "    paths =[]\n",
    "    for p in range(len(animalPaths)):\n",
    "        path = os.path.join(dataPath, animalPaths[p])\n",
    "        paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a2a90-774a-4641-888a-7e691018cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBootstrapResult(name, nBatch,ops, doMultiCorr=1):\n",
    "\n",
    "    boot_df = {}\n",
    "    for i in range(nBatch):\n",
    "        res = np.load(os.path.join(ops['outputPath'], name, name + '_' + str(i) + '.npy'),allow_pickle=True).item()\n",
    "        if i ==0:\n",
    "            if 'groups' in res:\n",
    "                groups = res['groups']\n",
    "            else:\n",
    "                groups = ops['areas']\n",
    "        res.pop('groups',None)\n",
    "        items = res.keys()\n",
    "        for j in items:\n",
    "            if i ==0:\n",
    "                boot_df[j] = res[j]\n",
    "            else:\n",
    "                boot_df[j] = np.concatenate((boot_df[j],res[j]),1)\n",
    "                  \n",
    "    pairs = list(itertools.combinations(range(0, len(groups)), 2))\n",
    "#     mat_alpha_coefficient= [[ 0.1, 0.05, .025,.01, .005, .001],[ 1.22, 1.36, 1.48, 1.63, 1.73, 1.95]]\n",
    "    \n",
    "#     # fig = plt.figure()    \n",
    "#     bins_Da = np.arange(0,1,0.02)\n",
    "#     sigLevel_ks = []\n",
    "#     for i in range(len(pairs)):\n",
    "#         CI_low = np.nanpercentile(boot_df['ks_boot'][i,:], 2.5)\n",
    "#         CI_high = np.nanpercentile(boot_df['ks_boot'][i,:], 97.5)\n",
    "    \n",
    "#         n1 = int(np.nanmedian(boot_df['sampleLength0'][i,:]))\n",
    "#         n2 = int(np.nanmedian(boot_df['sampleLength1'][i,:]))\n",
    "       \n",
    "#         vec_Da=[mat_alpha_coefficient[1][j] * np.sqrt((n1+n2)/(n1*n2)) for j in range(len(mat_alpha_coefficient[1]))]\n",
    "    \n",
    "#         if CI_low > vec_Da[5]:\n",
    "#             sigLevel_ks.append(3)\n",
    "#         elif CI_low > vec_Da[3]:\n",
    "#             sigLevel_ks.append(2)\n",
    "#         elif CI_low > vec_Da[1]:\n",
    "#             sigLevel_ks.append(1)\n",
    "#         else:\n",
    "#             sigLevel_ks.append(0)\n",
    "    \n",
    "    \n",
    "#     # fig = plt.figure() \n",
    "#     bins_rk = np.arange(1000,20000,1000)\n",
    "#     alphas = [0.1, 0.05, 0.01, 0.001]\n",
    "#     sigLevel_mannU= []\n",
    "#     for i in range(len(pairs)):\n",
    "    \n",
    "#         CI_low = np.nanpercentile(boot_df['rk_boot'][i,:], 2.5)\n",
    "#         CI_high = np.nanpercentile(boot_df['rk_boot'][i,:], 97.5)\n",
    "       \n",
    "#         n1 = int(np.nanmedian(boot_df['sampleLength0'][i,:]))\n",
    "#         n2 = int(np.nanmedian(boot_df['sampleLength1'][i,:])) \n",
    "        \n",
    "#         crit =[get_critical_ranksum(n1,n2, alphas[i]) for i in range(len(alphas))]\n",
    "    \n",
    "#         if CI_low > crit[3][0]:\n",
    "#             sigLevel_mannU.append(3)\n",
    "#         elif CI_low > crit[2][0]:\n",
    "#             sigLevel_mannU.append(2)\n",
    "#         elif CI_low > crit[1][0]:\n",
    "#             sigLevel_mannU.append(1)\n",
    "#         else:\n",
    "#             sigLevel_mannU.append(0)\n",
    "                   \n",
    "           # median_diff[i,k] = abs(np.median(boot_data1) - np.median(boot_data2))\n",
    "    from scipy.stats import percentileofscore\n",
    "    p_difference_quantiles_list = np.zeros(len(pairs),)\n",
    "    for i in range(len(pairs)):\n",
    "        diff_this = boot_df['median_diff'][i,:]\n",
    "        notNan = np.nonzero(np.isnan(diff_this) < 0.5)[0]\n",
    "        diff_notNan = diff_this[notNan]\n",
    "        target_value = 0\n",
    "        percentile = percentileofscore(diff_notNan, target_value, kind='rank')\n",
    "        quantile = percentile / 100\n",
    "        \n",
    "        p = 2*np.amin([quantile, 1-quantile])\n",
    "        p_difference_quantiles_list[i] = p\n",
    "        \n",
    "    if doMultiCorr:\n",
    "          p_difference_quantiles_list = statsmodels.stats.multitest.multipletests(p_difference_quantiles_list, method='fdr_bh')[1]  \n",
    "\n",
    "    \n",
    "    # ks_sigLevels_mat = np.empty((len(groups), len(groups))); ks_sigLevels_mat[:] = np.nan\n",
    "    # ks_distance_mat = np.empty((len(groups), len(groups))); ks_distance_mat[:] = np.nan\n",
    "    median_dist_mat = np.empty((len(groups), len(groups))); median_dist_mat[:] = np.nan\n",
    "    # mannU_sigLevels_mat = np.empty((len(groups), len(groups))); mannU_sigLevels_mat[:] = np.nan\n",
    "    p_difference_quantiles =  np.empty((len(groups), len(groups))); p_difference_quantiles[:] = np.nan\n",
    "    sigLevels_quantiles =  np.empty((len(groups), len(groups))); sigLevels_quantiles[:] = np.nan\n",
    "    for i in range(len(pairs)):\n",
    "        pos0 = pairs[i][0]\n",
    "        pos1 = pairs[i][1]\n",
    "      #  ks_sigLevels_mat[pos0,pos1] = sigLevel_ks[i]\n",
    "        #ks_distance_mat[pos1, pos0] =  np.nanmedian(boot_df['ks_boot'][i,:])\n",
    "       # mannU_sigLevels_mat[pos0,pos1] = sigLevel_mannU[i]\n",
    "        median_dist_mat[pos1,pos0] = np.nanmedian(abs(boot_df['median_diff'][i,:]))\n",
    "        \n",
    "        p = p_difference_quantiles_list[i]\n",
    "        p_difference_quantiles[pos0,pos1] = p\n",
    "        \n",
    "        if p < 0.001:\n",
    "            sigLevels_quantiles[pos0,pos1] = 3\n",
    "        elif p >= 0.001  and p < 0.01:\n",
    "            sigLevels_quantiles[pos0,pos1] = 2\n",
    "        elif p >= 0.01  and p < 0.05:\n",
    "            sigLevels_quantiles[pos0,pos1] = 1\n",
    "        elif p > 0.05:\n",
    "            sigLevels_quantiles[pos0,pos1] = 0\n",
    "                       \n",
    "    return median_dist_mat,p_difference_quantiles,sigLevels_quantiles,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c01d95-01e5-412f-992b-529836fafb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProportions_bySpatialBin_v3(df,binned_map, idx, thresh = 0, mask = 'none', V1_mask=[]):\n",
    "    # idx = top\n",
    "    chance = len(idx)/len(df)\n",
    "    \n",
    "    x = np.array(df['x'])\n",
    "    y = np.array(df['y'])\n",
    "    \n",
    "    binIndices = binned_map[y.astype(int),x.astype(int)]            \n",
    "    binIndices_unique = np.unique(binIndices)\n",
    "    \n",
    "    binned_mean_map = np.empty((binned_map.shape[0],binned_map.shape[1])); binned_mean_map[:] = np.nan\n",
    "\n",
    "    meanVal = []\n",
    "    for b in tqdm(binIndices_unique):\n",
    "        binPos = np.nonzero(binned_map == b)\n",
    "        binRange_y = np.arange(min(binPos[0]),max(binPos[0])+1)\n",
    "        binRange_x = np.arange(min(binPos[1]),max(binPos[1])+1)\n",
    "        \n",
    "        binCentre_y = int((binRange_y[0] + binRange_y[-1])/2)\n",
    "        binCentre_x = int((binRange_x[0] + binRange_x[-1])/2)\n",
    "        if mask == 'HVAs':\n",
    "            values_onMask = V1_mask[binCentre_y,binCentre_x] \n",
    "            if all(values_onMask == [227, 6, 19, 255]):\n",
    "                # print('Excluded V1 bin')\n",
    "                continue\n",
    "        elif mask == 'V1':\n",
    "            values_onMask = V1_mask[binCentre_y,binCentre_x] \n",
    "            if not all(values_onMask == [227, 6, 19, 255]):\n",
    "                # print('Excluded V1 bin')\n",
    "                continue\n",
    "        \n",
    "        idx_thisBin = []\n",
    "        for j in range(len(df)):\n",
    "            if df['x'].iloc[j] in binRange_x and df['y'].iloc[j] in binRange_y:\n",
    "                idx_thisBin.append(j)\n",
    "                \n",
    "        if len(idx_thisBin) > thresh:    \n",
    "            idx0_thisBin = np.intersect1d(idx, idx_thisBin)\n",
    "            prop_thisBin = (len(idx0_thisBin)/len(idx_thisBin))#/chance\n",
    "        else:\n",
    "            prop_thisBin = np.nan\n",
    "        binned_mean_map[min(binRange_y):max(binRange_y), min(binRange_x):max(binRange_x)] = prop_thisBin\n",
    "  \n",
    "    return binned_mean_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2081fcb-afb1-49b6-a80d-2e3d123f6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMeanValue_bySpatialBin_v2(df,binned_map, thresh = 0, varName = [], mask = 'V1', V1_mask=[]):\n",
    "    \n",
    "    x = np.array(df['x'])\n",
    "    y = np.array(df['y'])\n",
    "    \n",
    "    binIndices = binned_map[y.astype(int),x.astype(int)]            \n",
    "    binIndices_unique = np.unique(binIndices)\n",
    "    \n",
    "    if len(varName) > 0:\n",
    "        name = varName\n",
    "    else:\n",
    "        name = df.keys()[0]\n",
    "    \n",
    "    binned_mean_map = np.empty((binned_map.shape[0],binned_map.shape[1])); binned_mean_map[:] = np.nan\n",
    "\n",
    "    meanVal = []\n",
    "    for b in tqdm(binIndices_unique):\n",
    "        binPos = np.nonzero(binned_map == b)\n",
    "        binRange_y = np.arange(min(binPos[0]),max(binPos[0])+1)\n",
    "        binRange_x = np.arange(min(binPos[1]),max(binPos[1])+1)\n",
    "        \n",
    "        binCentre_y = int((binRange_y[0] + binRange_y[-1])/2)\n",
    "        binCentre_x = int((binRange_x[0] + binRange_x[-1])/2)\n",
    "        if mask == 'HVAs':\n",
    "            values_onMask = V1_mask[binCentre_y,binCentre_x] \n",
    "            if all(values_onMask == [227, 6, 19, 255]):\n",
    "                # print('Excluded V1 bin')\n",
    "                continue\n",
    "        elif mask == 'V1':\n",
    "            values_onMask = V1_mask[binCentre_y,binCentre_x] \n",
    "            if not all(values_onMask == [227, 6, 19, 255]):\n",
    "                # print('Excluded V1 bin')\n",
    "                continue\n",
    "\n",
    "        vals_thisBin = []\n",
    "        for j in range(len(df)):\n",
    "            if df['x'].iloc[j] in binRange_x and df['y'].iloc[j] in binRange_y:\n",
    "                vals_thisBin.append(df[varName].iloc[j])\n",
    "                \n",
    "        if len(vals_thisBin) > thresh:          \n",
    "            mean_thisBin = np.nanmean(np.array(vals_thisBin))\n",
    "        else:\n",
    "            mean_thisBin = np.nan\n",
    "        binned_mean_map[min(binRange_y):max(binRange_y), min(binRange_x):max(binRange_x)] = mean_thisBin\n",
    "  \n",
    "    return binned_mean_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77d541-931b-4bd6-ba25-e9bc0e1c57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critical_ranksum(n1, n2, alpha, tail = 'two-sided'):\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    # Mean of the U distribution\n",
    "    mean_u = n1 * n2 / 2\n",
    "\n",
    "    # Standard deviation of the U distribution\n",
    "    sd_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "\n",
    "    # Critical z-value for the desired alpha\n",
    "    if tail == 'two-sided':\n",
    "        z_critical = norm.ppf(1 - alpha / 2)\n",
    "    elif tail == 'one-sided':\n",
    "        z_critical = norm.ppf(1 - alpha)\n",
    "    else:\n",
    "        raise ValueError(\"tail must be 'two-sided' or 'one-sided'\")\n",
    "\n",
    "    # Critical value of U\n",
    "    u_critical_lower = mean_u - z_critical * sd_u\n",
    "    u_critical_upper = mean_u + z_critical * sd_u\n",
    "\n",
    "    return  u_critical_lower, u_critical_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fde8b9-cbd0-4d1d-bc87-46ea8b153113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLinearRegression(x,y):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    #remove nans\n",
    "    nanIdx1 = np.nonzero(np.isnan(x))[0] \n",
    "    nanIdx2 = np.nonzero(np.isnan(y))[0] \n",
    "    nanIdx = np.unique(np.concatenate((nanIdx1,nanIdx2),0))\n",
    "    x = np.delete(x,nanIdx).reshape((-1,1))\n",
    "    y = np.delete(y,nanIdx)\n",
    "     \n",
    "     # #try removing non-decimal numbers from y, since they are likely to be faulty gaussian fits\n",
    "     # good = np.nonzero(np.mod(y,1) > 0)[0] \n",
    "     # x = x[good]     \n",
    "     # y = y[good]\n",
    "     \n",
    "    reg = LinearRegression().fit(x, y) #fit\n",
    "     \n",
    "    r2 = reg.score(x,y)  #get fit parameters\n",
    "    intercept = reg.intercept_\n",
    "    slope = reg.coef_\n",
    "    \n",
    "    x_vals = np.arange(min(x),max(x), 0.01) #make line from slope and intercept for plotting\n",
    "    y_vals = intercept + slope*x_vals\n",
    "    \n",
    "    corr, pVal_corr = stats.pearsonr(np.squeeze(x),np.squeeze(y))\n",
    "    \n",
    "    result_dict = {'r2': r2,\n",
    "                   'intercept': intercept,\n",
    "                   'slope': slope,\n",
    "                   'x_vals': x_vals,\n",
    "                   'y_vals': y_vals,\n",
    "                   'corr': corr,\n",
    "                   'pVal_corr': pVal_corr}\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20f9ba-a539-485d-ac72-53109d1d553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLinearRegression_withCI(x,y):\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    nanIdx1 = np.nonzero(np.isnan(x))[0] \n",
    "    nanIdx2 = np.nonzero(np.isnan(y))[0] \n",
    "    nanIdx = np.unique(np.concatenate((nanIdx1,nanIdx2),0))\n",
    "    x = np.delete(x,nanIdx).reshape((-1,1))\n",
    "    y = np.delete(y,nanIdx)\n",
    "\n",
    "    X = sm.add_constant(x)  # Adds intercept term\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    x_vals = np.arange(min(x),max(x), 0.1) #make line from slope and intercept for plotting\n",
    "    X_pred = sm.add_constant(x_vals)\n",
    "    predictions = results.get_prediction(X_pred)\n",
    "    pred_summary = predictions.summary_frame(alpha=0.05)  # 95% CI\n",
    "\n",
    "    corr, pVal_corr = stats.pearsonr(np.squeeze(x),np.squeeze(y))\n",
    "    intercept = results.params[0]\n",
    "    slope =  results.params[1]\n",
    "    \n",
    "    result_dict = {'r2': results.rsquared,\n",
    "                   'intercept': intercept,\n",
    "                   'slope': slope,\n",
    "                   'x_vals': x_vals,\n",
    "                   'y_vals':np.array(pred_summary['mean']),\n",
    "                   'corr': corr,\n",
    "                   'pVal_corr': pVal_corr, \n",
    "                   'ci_lower':np.array(pred_summary[\"mean_ci_lower\"]),\n",
    "                   'ci_upper':np.array(pred_summary[\"mean_ci_upper\"])}\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728414a2-ab3e-48c3-88eb-ff50961c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_spatialBins(binned_values_map, spatialBin =300, nSmoothBins=1):\n",
    "    \n",
    "    nY,nX = binned_values_map.shape\n",
    "    \n",
    "    um_per_pixel = 9 #this is from measuring the WF FOV size\n",
    "     \n",
    "    spatialBin_px = int(spatialBin/um_per_pixel)\n",
    "    smooth_range = spatialBin_px*nSmoothBins\n",
    "    \n",
    "    binned_vals_map_smooth = binned_values_map.copy()\n",
    "    for x in range(nX):\n",
    "        xRange = np.arange(x-smooth_range-1, x+smooth_range +1)\n",
    "        if any(xRange < 0):\n",
    "            xRange =  np.arange(0, x+smooth_range+1)\n",
    "        if any(xRange >= nX):\n",
    "            xRange =  np.arange(x-smooth_range-1, nX-1)\n",
    "            \n",
    "        for y in range(nY):\n",
    "            if not np.isnan(binned_values_map[y,x]):\n",
    "                yRange = np.arange(y-smooth_range-1, y+smooth_range+1)\n",
    "                if any(yRange < 0):\n",
    "                    yRange =  np.arange(0, y+smooth_range+1)\n",
    "                if any(yRange >= nY):\n",
    "                    yRange =  np.arange(y-smooth_range-1, nY-1)\n",
    "                    \n",
    "                vals0 = binned_values_map[yRange,:]\n",
    "                vals = vals0[:,xRange]\n",
    "                uniqueVals = np.unique(vals)\n",
    "                \n",
    "                smoothVal = np.nanmean(uniqueVals)\n",
    "                \n",
    "                binned_vals_map_smooth[y,x] = smoothVal\n",
    "                \n",
    "    return binned_vals_map_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41850c4a-d69b-4549-9713-1cf7672b0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinValues(binned_map, binned_map_values, map_colors, colors_LUT):\n",
    "    #binned_map_values = binned_prop_map_centre\n",
    "    bins_unique = np.unique(binned_map)\n",
    "\n",
    "   \n",
    "    bins, values, positions = [],[],[]\n",
    "    for b in range(len(bins_unique)):\n",
    "        idx = np.nonzero(binned_map == bins_unique[b])\n",
    "        y,x = np.mean(idx[0]), np.mean(idx[1])\n",
    "        positions.append([x,y])\n",
    "        \n",
    "        bins.append(binned_map[int(y),int(x)])\n",
    "        values.append(binned_map_values[int(y),int(x)])\n",
    "        \n",
    "    bins_df =pd.DataFrame({'binIdx': np.array(bins), \n",
    "                           'values': np.array(values)})\n",
    "    \n",
    "    binAreas = []\n",
    "    for roi in range(len(positions)):\n",
    "        color = map_colors[int(positions[roi][1]), int(positions[roi][0])]\n",
    "        \n",
    "        f = [color == colors_LUT['colors'][i] for i in range(len(colors_LUT['colors']))]\n",
    "        f = [all(f[i]) for i in range(len(f))]\n",
    "        foundIt = any(np.array(f)) \n",
    "        \n",
    "        if foundIt:\n",
    "            idx = np.nonzero(np.array(f) > 0.5)[0]\n",
    "            area = colors_LUT['areas'][int(idx)]\n",
    "        else:\n",
    "            if np.sum(color) == 765:\n",
    "                area = 'OUT'\n",
    "            else: #if color not an area color and not white, its on the border. Search for closest color that matches an area color and assign that area\n",
    "                increments = np.arange(1,5)\n",
    "                cnt = 0\n",
    "                for incr in increments: #search around the centre pixel with increasing radius\n",
    "                    #incr =1\n",
    "                    yVals = np.arange(positions[roi][1] - incr,positions[roi][1] + incr +1)\n",
    "                    xVals = np.arange(positions[roi][0] - incr,positions[roi][0] + incr +1)\n",
    "                    \n",
    "                    x = np.repeat(xVals,len(yVals))\n",
    "                    y = np.tile(yVals,len(xVals))\n",
    "                    \n",
    "                    pixels = np.array([x,y])                        \n",
    "                    for p in range(len(pixels)):\n",
    "                        thisCol = map_colors[int(pixels[1,p]), int(pixels[0,p])]\n",
    "                        f = [thisCol == colors_LUT['colors'][i] for i in range(len(colors_LUT['colors']))]\n",
    "                        f = [all(f[i]) for i in range(len(f))]\n",
    "                        foundIt = any(np.array(f)) \n",
    "                        if foundIt:\n",
    "                            idx = np.nonzero(np.array(f) > 0.5)[0]\n",
    "                            area = colors_LUT['areas'][int(idx)]\n",
    "                            cnt = 1\n",
    "                            break\n",
    "                    \n",
    "                    if cnt:\n",
    "                        break\n",
    "        \n",
    "        binAreas.append(area)\n",
    "        \n",
    "         \n",
    "    bins_df =pd.DataFrame({'binIdx': np.array(bins), \n",
    "                           'values': np.array(values),\n",
    "                           'binArea': np.array(binAreas)})\n",
    "    \n",
    "    return bins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d90aa-97c7-4fb2-abed-e690d37f6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap'):\n",
    "    '''\n",
    "    Function to offset the \"center\" of a colormap. Useful for\n",
    "    data with a negative min and positive max and you want the\n",
    "    middle of the colormap's dynamic range to be at zero.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "      cmap : The matplotlib colormap to be altered\n",
    "      start : Offset from lowest point in the colormap's range.\n",
    "          Defaults to 0.0 (no lower offset). Should be between\n",
    "          0.0 and `midpoint`.\n",
    "      midpoint : The new center of the colormap. Defaults to \n",
    "          0.5 (no shift). Should be between 0.0 and 1.0. In\n",
    "          general, this should be  1 - vmax / (vmax + abs(vmin))\n",
    "          For example if your data range from -15.0 to +5.0 and\n",
    "          you want the center of the colormap at 0.0, `midpoint`\n",
    "          should be set to  1 - 5/(5 + 15)) or 0.75\n",
    "      stop : Offset from highest point in the colormap's range.\n",
    "          Defaults to 1.0 (no upper offset). Should be between\n",
    "          `midpoint` and 1.0.\n",
    "    '''\n",
    "    cdict = {\n",
    "        'red': [],\n",
    "        'green': [],\n",
    "        'blue': [],\n",
    "        'alpha': []\n",
    "    }\n",
    "\n",
    "    # regular index to compute the colors\n",
    "    reg_index = np.linspace(start, stop, 257)\n",
    "\n",
    "    # shifted index to match the data\n",
    "    shift_index = np.hstack([\n",
    "        np.linspace(0.0, midpoint, 128, endpoint=False), \n",
    "        np.linspace(midpoint, 1.0, 129, endpoint=True)\n",
    "    ])\n",
    "\n",
    "    for ri, si in zip(reg_index, shift_index):\n",
    "        r, g, b, a = cmap(ri)\n",
    "\n",
    "        cdict['red'].append((si, r, r))\n",
    "        cdict['green'].append((si, g, g))\n",
    "        cdict['blue'].append((si, b, b))\n",
    "        cdict['alpha'].append((si, a, a))\n",
    "\n",
    "    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n",
    "    plt.register_cmap(cmap=newcmap)\n",
    "\n",
    "    return newcmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebed291-6ceb-44d4-8324-1c43e38e2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSessionReference(df,varName=[]):\n",
    "    # idx = green_aud_selective\n",
    "    # cluster_labels = k_means_labels\n",
    "    sessionIdx = df['sessionIdx'].unique()  \n",
    "    #\n",
    "    dorsal = ['AM', 'PM', 'A', 'RL'] \n",
    "    ventral = ['P', 'POR', 'LI', 'LM', 'AL']\n",
    "    \n",
    "    abs_index, rel_index = [],[]\n",
    "    seshX, seshY, seshAzi, seshElev, seshAreas, seshAnimal, seshSource, seshIdx, seshVar, seshStream, seshBatch, seshMapGood = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "    pos_DV, pos_AP, prop_ventral = [],[],[]\n",
    "    for sesh in range(len(sessionIdx)):\n",
    "        session = sessionIdx[sesh]\n",
    "              \n",
    "        idx_thisSession_rel = np.nonzero(np.array(df['sessionIdx']) == session)[0]\n",
    "        df_thisSession = df.iloc[idx_thisSession_rel]\n",
    "        idx_thisSession_abs = np.array(df_thisSession.index)   \n",
    "                  \n",
    "        rel_index.append(idx_thisSession_rel)\n",
    "        abs_index.append(idx_thisSession_abs)\n",
    "        \n",
    "        #area\n",
    "        theseAreas = np.array(df_thisSession['area'])\n",
    "        areas1, counts = np.unique(theseAreas, return_counts=True)\n",
    "                   \n",
    "        if 'OUT' in areas1:\n",
    "            t = np.nonzero(areas1 == 'OUT')[0]\n",
    "            areas1 = np.delete(areas1, t)\n",
    "            counts = np.delete(counts, t)\n",
    "        \n",
    "        if len(areas1) > 0:  \n",
    "            this = areas1[np.argmax(counts)]\n",
    "        else:\n",
    "            this = 'OUT'\n",
    "            \n",
    "        seshAreas.append(this)\n",
    "            \n",
    "        if this in dorsal:\n",
    "            seshStream.append('Dorsal')\n",
    "        elif this in ventral:\n",
    "            seshStream.append('Ventral')\n",
    "        elif this =='V1':\n",
    "            seshStream.append('V1')\n",
    "        else:\n",
    "            seshStream.append('OUT')\n",
    "                       \n",
    "            \n",
    "        #retinotopy\n",
    "        if df_thisSession['isMapGood'].iloc[0] == 0:\n",
    "            seshAzi.append(np.nanmedian(np.array(df_thisSession['azi'])))\n",
    "            seshElev.append(np.nanmedian(np.array(df_thisSession['elev'])))\n",
    "        else:\n",
    "            seshAzi.append(np.nanmedian(np.array(df_thisSession['azi_orig'])))\n",
    "            seshElev.append(np.nanmedian(np.array(df_thisSession['elev_orig'])))\n",
    "            \n",
    "        #anat.loc.\n",
    "        seshX.append(np.nanmean(np.array(df_thisSession['x'])))\n",
    "        seshY.append(np.nanmean(np.array(df_thisSession['y'])))\n",
    "        \n",
    "        #animal\n",
    "        animal = np.array(df_thisSession['animal'])[0]\n",
    "        seshAnimal.append(animal)\n",
    "        \n",
    "        #batch\n",
    "        if animal < 149:\n",
    "            batch = 1\n",
    "        else:\n",
    "            batch = 2\n",
    "        seshBatch.append(batch)\n",
    "        \n",
    "        if len(varName) >0:\n",
    "            seshVar.append(np.nanmean(np.array(df_thisSession[varName])))\n",
    "        \n",
    "        #source\n",
    "        # source = np.array(df_thisSession['source'])[0]\n",
    "        # seshSource.append(source)\n",
    "        \n",
    "        #\n",
    "        if 'pos_DV' in df_thisSession.keys():\n",
    "            pos_DV.append(df_thisSession['pos_DV'].iloc[0])\n",
    "        if 'pos_DV' in df_thisSession.keys():\n",
    "            pos_AP.append(df_thisSession['pos_AP'].iloc[0])\n",
    "        if 'prop_ventral' in df_thisSession.keys():\n",
    "            prop_ventral.append(df_thisSession['prop_ventral'].iloc[0])\n",
    "          \n",
    "        \n",
    "        #sessionIdx \n",
    "        sessionIdx0 = np.array(df_thisSession['sessionIdx'])[0]\n",
    "        seshIdx.append(sessionIdx0)\n",
    "        \n",
    "        #map good\n",
    "        seshMap = np.array(df_thisSession['isMapGood'])[0]\n",
    "        seshMapGood.append(seshMap)\n",
    "\n",
    "    sessionRef = {'abs_index' : abs_index,\n",
    "              'rel_index' : rel_index,\n",
    "              'seshAzi' : seshAzi,\n",
    "              'seshElev' : seshElev,\n",
    "              'seshX' : seshX,\n",
    "              'seshY' : seshY,\n",
    "              'seshAreas': seshAreas, \n",
    "              'seshAnimal': seshAnimal, \n",
    "               'myVar': seshVar, \n",
    "               'seshStream': seshStream,\n",
    "               'seshBatch':np.array(seshBatch),\n",
    "              # 'seshSource': seshSource,#\n",
    "              'seshIdx': seshIdx, \n",
    "              'seshMapGood': seshMapGood, \n",
    "              'prop_ventral': prop_ventral,\n",
    "              'pos_DV': pos_DV,\n",
    "              'pos_AP': pos_AP}\n",
    "            \n",
    "    return sessionRef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b2a35-98dc-4a9f-ac7f-2e604c9d9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProportions_bySession_v2(df, ref_df, thresh =10):\n",
    "     \n",
    "   # df = df_sel_azi\n",
    "   # ref_df = df_green_aud_resp\n",
    "    \n",
    "    sessions = ref_df['sessionIdx'].unique()\n",
    "    prop = []\n",
    "    for sesh in sessions:\n",
    "        ref_thisSession = ref_df[ref_df['sessionIdx'] == sesh]\n",
    "        df_thisSession = df[df['sessionIdx'] == sesh]\n",
    "        if len(df_thisSession) < thresh:\n",
    "            thisProp = np.nan\n",
    "        else:\n",
    "            thisProp = len(df_thisSession)/len(ref_thisSession)\n",
    "        prop.append(thisProp)\n",
    "        \n",
    "    return prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d61de8-17f0-4150-9946-edac74beddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignAreaToSession(df, policy='mostRois'):\n",
    "   # df = df_green_aud\n",
    "    sessions = df['sessionIdx'].unique()\n",
    "    \n",
    "    areas, animals = [],[]\n",
    "    for sesh in sessions:\n",
    "        df_thisSession = df[df['sessionIdx'] == sesh]\n",
    "        animals.append(int(df_thisSession['animal'].unique()))\n",
    "        area_thisSession, counts = np.unique(np.array(df_thisSession['area']), return_counts=True)\n",
    "        \n",
    "        if 'OUT' in area_thisSession:\n",
    "            t = np.nonzero(area_thisSession == 'OUT')[0]\n",
    "            area_thisSession = np.delete(area_thisSession, t)\n",
    "            counts = np.delete(counts, t)\n",
    "        \n",
    "        if len(area_thisSession) > 0:\n",
    "            if policy == 'mostRois':\n",
    "                area = area_thisSession[np.argmax(counts)]\n",
    "                areas.append(area)\n",
    "        else:\n",
    "            areas.append('OUT')\n",
    "\n",
    "    areas = np.array(areas)\n",
    "    \n",
    "    areaBySession = {'areas': areas,\n",
    "                     'sessionIdx': sessions,\n",
    "                     'animals': animals}\n",
    "    \n",
    "    return areaBySession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb833e-4504-4163-ac8b-67050f0b9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideSessionsByArea(prop_bySession, areas, areaBySession):\n",
    "    #prop_bySession= green_aud_prop_sel  \n",
    "    #areaBySession = areas_green_aud\n",
    "\n",
    "    prop_byArea = []\n",
    "    for area in areas:\n",
    "        t = np.nonzero(areaBySession['areas'] == area)[0]\n",
    "        theseProps = np.array([prop_bySession[t[i]] for i in range(len(t))])\n",
    "        prop_byArea.append(theseProps)\n",
    "        \n",
    "    return prop_byArea  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c952165-857a-4dac-9706-f55653ddf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyFreqSplines(freq_curve, threshold):\n",
    "    nRois = freq_curve.shape[0]\n",
    "    \n",
    "    doublePeak, singlePeak = [],[]\n",
    "    for roi in tqdm(range(nRois)):\n",
    "        freq = freq_curve[roi,:]\n",
    "        \n",
    "        if all(freq < 0):\n",
    "            freq = abs(freq)\n",
    "        #actual spline fitting:\n",
    "        maxVal = max(freq)\n",
    "        thresh = maxVal*threshold #7\n",
    "        \n",
    "        aboveThresh = np.nonzero(freq > thresh)[0]\n",
    "        jump = np.diff(aboveThresh)\n",
    "        if len(aboveThresh) ==1:\n",
    "            singlePeak.append(roi)\n",
    "        else:\n",
    "            if np.max(jump) > 1: #double peaked\n",
    "                doublePeak.append(roi)\n",
    "            else:\n",
    "                singlePeak.append(roi)\n",
    "\n",
    "    singlePeak = np.array(singlePeak) \n",
    "    doublePeak = np.array(doublePeak)    \n",
    "    \n",
    "    return singlePeak,doublePeak    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dc814-1a25-4731-a01f-c5bc0e599a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMannWhitneyU_forBoxplots(data, multiComp = 'fdr'):\n",
    "    #data will be a list\n",
    "    \n",
    "    nBox = len(data)\n",
    "    \n",
    "    pVals, compIdx = [],[]\n",
    "    for b in range(nBox):\n",
    "        for b1 in range(nBox):\n",
    "            if b < b1:           \n",
    "                if len(data[b]) > 0 and len(data[b1]) > 0:\n",
    "                    t, p = stats.mannwhitneyu(data[b],data[b1])\n",
    "                    pVals.append(p)\n",
    "                else:\n",
    "                    pVals.append(np.nan)\n",
    "                compIdx.append(str(b) + '_' + str(b1))\n",
    "\n",
    "    n_comp = len(compIdx)\n",
    "    pVals = np.array(pVals)\n",
    "    \n",
    "    if multiComp == 'bonferroni':\n",
    "        pVals_adj = pVals*n_comp\n",
    "    elif multiComp == 'fdr':       \n",
    "        pVals_adj = statsmodels.stats.multitest.multipletests(pVals, method='fdr_bh')[1]   \n",
    "        # pVals_adj = statsmodels.stats.multitest.multipletests(pVals, method='hs')[1]                        \n",
    "    else:\n",
    "        pVals_adj = pVals\n",
    "\n",
    "    return pVals_adj, compIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08151ea5-2162-4486-a702-3b93774b4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElevation_greenAud(df, maps, peak, onlyPeakSide = 1):\n",
    "    # maps = maps_green_aud_sig\n",
    "    # df_fit = df_fit_1d_green_aud_full\n",
    "    # peak = df_fit['gaussian_peak']\n",
    "\n",
    "    leftBorder = 4.4\n",
    "    rightBorder = 7.6\n",
    "\n",
    "    left_tuned = np.nonzero(peak < leftBorder)[0]\n",
    "    right_tuned = np.nonzero(peak > rightBorder)[0]\n",
    "    centre_tuned0 = np.setdiff1d(np.arange(0,len(peak)), left_tuned)\n",
    "    centre_tuned1 = np.setdiff1d(np.arange(0,len(peak)), right_tuned)\n",
    "    centre_tuned = np.intersect1d(centre_tuned0, centre_tuned1)\n",
    "    \n",
    "    elevPeak = np.zeros(len(df),)\n",
    "    \n",
    "    if onlyPeakSide:\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            thisMap = maps[i,::]\n",
    "            \n",
    "            if df['animal'].iloc[i] < 149:\n",
    "                if i in left_tuned:\n",
    "                    this = np.nanmean(thisMap[0:3],0)\n",
    "                elif i in centre_tuned:\n",
    "                    this =thisMap[3,:]\n",
    "                elif i in right_tuned:\n",
    "                    this = np.nanmean(thisMap[4:7],0)\n",
    "            else:\n",
    "                if i in left_tuned:\n",
    "                    this = np.nanmean(thisMap[0:5],0)\n",
    "                elif i in centre_tuned:\n",
    "                    this = np.nanmean(thisMap[5:8],0)\n",
    "                elif i in right_tuned:\n",
    "                    this = np.nanmean(thisMap[8::],0)    \n",
    "            \n",
    "            elevPeak[i] = np.argmax(this)\n",
    "    else:\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            thisMap = maps[i,::]\n",
    "            this = np.nanmean(thisMap,0)\n",
    "            elevPeak[i] = np.argmax(this)\n",
    "        \n",
    "    \n",
    "    elevPeak[elevPeak ==0] = 4   \n",
    "    elevPeak[elevPeak ==2] = 0      \n",
    "    elevPeak[elevPeak ==1] = 2  \n",
    "    \n",
    "    return elevPeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c8538-f5a3-4546-ae6f-74e02989554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparsityIdx(maps):\n",
    "#     def sparseIdx(responses):\n",
    "#         si =  1-(np.nansum(responses/len(responses)))**2/np.sum(((responses**2)/len(responses)))   \n",
    "    \n",
    "#         return si\n",
    "#     def sparseIdx_inv_simple(responses):\n",
    "#         si0 = ((np.nansum(responses)/len(responses))**2)/(np.nansum(responses**2)/len(responses))\n",
    "#         si = (1-si0)\n",
    "#         return si\n",
    "    def sparseIdx_inv(responses):\n",
    "        si0 =  (np.nansum(responses/len(responses)))**2/np.nansum(((responses**2)/len(responses)))  \n",
    "        si = (1-si0)/(1-(1/len(responses)))\n",
    "        return si\n",
    "    # maps = maps_freq_sig\n",
    "    freqs = maps\n",
    "  \n",
    "    # freqs = np.nanmean(maps[:,1:11,:],2)\n",
    "    # freqs = maps[:,1:12,:]\n",
    "    freqs = np.reshape(freqs, (freqs.shape[0], freqs.shape[1]*freqs.shape[2]))\n",
    "  \n",
    "    nRois = freqs.shape[0]\n",
    "            \n",
    "    all_si = []\n",
    "    for roi in range(nRois):\n",
    "        these = freqs[roi,:]\n",
    "        these[these < 0] = 0\n",
    "        si = sparseIdx_inv(these)\n",
    "        all_si.append(si)\n",
    "    all_si = np.array(all_si)\n",
    "    \n",
    "    return all_si\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb0fc0-a4de-49bf-b248-3993d7205e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBootstrapDiffP(diff):        \n",
    "    sort_diff = np.sort(diff)\n",
    "    firstAbove = np.nonzero(sort_diff < 0)[0]\n",
    "    if len(firstAbove) == 0:\n",
    "        pVal = '< ' + str(1/len(diff))\n",
    "    else:\n",
    "        pVal = str(len(firstAbove)/len(diff))\n",
    "        \n",
    "    return pVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e673b-6193-4f18-9853-caca7e2547f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateAzimuth_coliseum(df):\n",
    "    idx_0 = np.nonzero(np.array(df['animal']).astype(int) < 149)[0]\n",
    "    idx_1 = np.nonzero(np.array(df['animal']).astype(int) >= 149)[0]\n",
    "    \n",
    "    df_0 = df.iloc[idx_0]\n",
    "    df_1 = df.iloc[idx_1]\n",
    "    \n",
    "    peakAzi_0 = np.array(df_0['aziPeak'])\n",
    "    \n",
    "    max0 = int(np.round(max(df_0['aziPeak'])))\n",
    "    min0 = 0\n",
    "    small_range = np.linspace(min0, max0, int(max0*100))\n",
    "    max1 = int(np.round(max(df_1['aziPeak'])))\n",
    "    min1 =0\n",
    "    large_range = np.linspace(min1, max1, int(max0*100))\n",
    "    \n",
    "    data_interp = np.array([np.interp(peakAzi_0[k],small_range,large_range) for k in range(len(peakAzi_0))])\n",
    "    df_0['aziPeak'] = data_interp\n",
    "    \n",
    "    if 'elevPeak' in df.keys():\n",
    "        peakElev_0 = np.array(df_0['elevPeak']) \n",
    "        \n",
    "        max0 = int(np.round(max(df_0['elevPeak'])))\n",
    "        min0 = 0\n",
    "        small_range = np.linspace(min0, max0, int(max0*100))\n",
    "        max1 = int(np.round(max(df_1['elevPeak'])))\n",
    "        min1 =0\n",
    "        large_range = np.linspace(min1, max1, int(max0*100))\n",
    "    \n",
    "        data_interp = np.array([np.interp(peakElev_0[k],small_range,large_range) for k in range(len(peakElev_0))])\n",
    "        df_0['elevPeak'] = data_interp\n",
    "\n",
    "    df_interp = pd.concat([df_0, df_1])\n",
    "\n",
    "    return df_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c53e8f-2a3b-418f-995e-5129a679200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDict_fromMatlabStruct(mat_file, struct_name):\n",
    "    \n",
    "    # Access the MATLAB structure by its name\n",
    "    mat_struct = mat_file[struct_name]\n",
    "\n",
    "    # Convert the MATLAB structure to a Python dictionary\n",
    "    py_dict = {name: np.array(mat_struct[name][0]) for name in mat_struct.dtype.names}\n",
    "    \n",
    "    py_dict2 = {name: np.array(py_dict[name][0]) for name in py_dict.keys()}\n",
    "\n",
    "    return py_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4818d91-1176-41bb-9178-fd6f5fde7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWilcoxon_forBoxplots(data, multiComp = 'hs'):\n",
    "    # Data should be a numpy array with the following shape: CatagoricalVar x observationsPerCategory\n",
    "    #data = green_vis_prop_resp\n",
    "    \n",
    "    nBox, nObservations = data.shape\n",
    "    \n",
    "    pVals, compIdx = [],[]\n",
    "    for b in range(nBox):\n",
    "        for b1 in range(nBox):\n",
    "            if b < b1:         \n",
    "                goodIdx_1= np.nonzero(~np.isnan(data[b,:]))[0]\n",
    "                goodIdx_2= np.nonzero(~np.isnan(data[b1,:]))[0]\n",
    "                goodIdx = np.intersect1d(goodIdx_1, goodIdx_2)\n",
    "                \n",
    "                t, p = stats.wilcoxon(data[b,goodIdx],data[b1,goodIdx],zero_method = 'pratt')\n",
    "                pVals.append(p)\n",
    "                compIdx.append(str(b) + '_' + str(b1))\n",
    "            \n",
    "    n_comp = len(compIdx)\n",
    "    pVals = np.array(pVals)\n",
    "    \n",
    "    if multiComp == 'bonferroni':\n",
    "        pVals_adj = pVals*n_comp\n",
    "    elif multiComp == 'hs':       \n",
    "        pVals_adj = statsmodels.stats.multitest.multipletests(pVals, method='hs')[1]  \n",
    "    elif multiComp == 'fdr':       \n",
    "        pVals_adj = statsmodels.stats.multitest.multipletests(pVals, method='fdr_bh')[1]   \n",
    "    else:\n",
    "        pVals_adj = pVals\n",
    "\n",
    "    return pVals_adj, compIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545eff7-4130-4d16-8214-cdbafc6c0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPlotSettings_splitAxis(fig,ax,ytitle,xtitle,title,axisColor = 'k', mySize=7, myAxisSize =5):\n",
    "    from matplotlib import font_manager\n",
    "\n",
    "    font_dirs = ['C:\\\\Users\\\\egeaa\\\\Desktop\\\\myFonts']\n",
    "    font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "    \n",
    "    for font_file in font_files:\n",
    "        font_manager.fontManager.addfont(font_file)\n",
    "    myFont = 'Arial'\n",
    "    # mySize = 7 #18 for posters\n",
    "    ax.spines['left'].set_color(axisColor)\n",
    "    ax.spines['bottom'].set_color(axisColor)\n",
    "    ax.xaxis.label.set_color(axisColor)\n",
    "    ax.yaxis.label.set_color(axisColor)\n",
    "    ax.tick_params(axis='x', colors=axisColor)\n",
    "    ax.tick_params(axis='y', colors=axisColor)\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = myFont\n",
    "    # plt.rcParams[\"font.family\"] = myFont\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = mySize\n",
    "    # ax.set_ylabel(ytitle)\n",
    "    # ax.set_xlabel(xtitle)\n",
    "    # ax.set_title(title,weight = 'bold')\n",
    "    ax.set_ylabel(ytitle, fontname=myFont, fontsize=mySize, labelpad = 1)\n",
    "    ax.set_xlabel(xtitle, fontname=myFont, fontsize=mySize)\n",
    "    ax.set_title(title, fontname=myFont, fontsize=mySize, weight = 'bold')\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontname(myFont)\n",
    "        tick.set_fontsize(myAxisSize)        \n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontname(myFont)\n",
    "        tick.set_fontsize(myAxisSize)    \n",
    "    right = ax.spines[\"right\"]\n",
    "    right.set_visible(False)\n",
    "    top = ax.spines[\"top\"]\n",
    "    top.set_visible(False) \n",
    "    # for axis in ['top','bottom','left','right']:\n",
    "    #     ax.spines[axis].set_linewidth(0.5)\n",
    "    ax.tick_params(width=0.25)\n",
    "    for line in [\"left\",\"bottom\"]:\n",
    "        ax.spines[line].set_linewidth(0.25)\n",
    "        ax.spines[line].set_position((\"outward\",3))\n",
    "        # ax.spines['bottom'].set_position(('data', 7)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
